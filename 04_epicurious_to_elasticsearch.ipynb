{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Epicurious JSON to Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import JSON into Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Bitnami Cluster on Google Compute Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure a [Bitnami Cluster on Google Compute Engine](https://console.cloud.google.com/marketplace/details/bitnami-launchpad/elasticsearch-cluster) with the number of nodes depending on your project.\n",
    "\n",
    "SSH into the cluster. Transfer your JSON from local to the primary instance (either using *gcloud*, hosting and *wget* ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert JSON to new-line-delimited JSON\n",
    "\n",
    "Elasticsearch expects new-line delimited JSON file for bulk import. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cat recipe_urls_final_v3.json | jq -c '.[]' > recipe_urls_final_v4.ndjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the lines need to follow the Bulk API format (add index information before every line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sed -e 's/^/{\"index\":{}\\n/' recipe_urls_final_v4.ndjson > recipe_urls_final_v5.ndjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push to Elasticsearch\n",
    "\n",
    "Use bulk import facilities: Put index and set \"read_only_allow_delete\" to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    curl -XPUT 'http://localhost:9200/recipes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    curl -XPUT -H \"Content-Type: application/json\" http://localhost:9200/recipes/_settings -d '{\"index.blocks.read_only_allow_delete\": false}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to push the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    curl -s -XPOST localhost:9200/recipes/recipe/_bulk -H \"Content-Type: application/x-ndjson\" --data-binary @recipe_urls_final_v5.ndjson > elastic_logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's list all indices and check if we have pushed the data successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    curl -X GET \"localhost:9200/_cat/indices?v\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be: \n",
    "    \n",
    "        health status index   [...]  pri rep docs.count docs.deleted store.size pri.store.size\n",
    "        green  open   recipes [...]    5   1      35768            0    183.9mb         92.2mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of data cleaning was done in \"03_elasticsearch_data_preperation\" to convert the base JSON to a format that is schema-able by Elasticsearch. Still, somewhere during migration, two poor recipes were separated from the crowd. :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annex: Configuration of Elasticsearch Cluster on Google Cloud (Bitnami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Google App Engine, you can directly connect with the cluster with the node's private IP, **given that**:\n",
    "- you are using Google App Engine on flex environment\n",
    "- that the application is actually **deployed** (connection will fail in web preview)\n",
    "\n",
    "Also, in Google App Engine environments, it turns out to be necessary to send ES get requests with body as post, see also [Python Elasticsearch Documentation](https://elasticsearch-py.readthedocs.io/en/master/).\n",
    "\n",
    "Therefore, we get the following connection in our Python code:\n",
    "\n",
    "    es = Elasticsearch('http://10.128.0.10:9200', send_get_body_as='POST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSHing into your nodes, you can always check the connection health. If everything worked out, you should see \"green\" as output.\n",
    "\n",
    "    curl -XGET 'localhost:9200/_cluster/health?pretty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be similar to: \n",
    "    \n",
    "    {\n",
    "      \"cluster_name\" : \"es-cluster\",\n",
    "      \"status\" : \"green\",\n",
    "      \"timed_out\" : false,\n",
    "      \"number_of_nodes\" : 3,\n",
    "      \"number_of_data_nodes\" : 3,\n",
    "      \"active_primary_shards\" : 5,\n",
    "      \"active_shards\" : 10,\n",
    "      \"relocating_shards\" : 0,\n",
    "      \"initializing_shards\" : 0,\n",
    "      \"unassigned_shards\" : 0,\n",
    "      \"delayed_unassigned_shards\" : 0,\n",
    "      \"number_of_pending_tasks\" : 0,\n",
    "      \"number_of_in_flight_fetch\" : 0,\n",
    "      \"task_max_waiting_in_queue_millis\" : 0,\n",
    "      \"active_shards_percent_as_number\" : 100.0\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
