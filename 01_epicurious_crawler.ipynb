{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "\n",
    "#### *Extracting all >35.000 recipe URLs from Epicurious*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Base URL List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.epicurious.com/search?content=recipe&page=1985',\n",
       " 'https://www.epicurious.com/search?content=recipe&page=1986',\n",
       " 'https://www.epicurious.com/search?content=recipe&page=1987',\n",
       " 'https://www.epicurious.com/search?content=recipe&page=1988',\n",
       " 'https://www.epicurious.com/search?content=recipe&page=1989']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.epicurious.com/search?content=recipe&page=\" \n",
    "urls = []\n",
    "\n",
    "for i in range(1989): \n",
    "    urls.append(base_url+(str(i+1)))\n",
    "urls[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle/recipe_base_urls.pkl', 'wb') as f:\n",
    "    pickle.dump(urls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Core URL List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_string = \"https://www.epicurious.com/search?content=recipe&page=1985\"\n",
    "page = urlopen(page_string)\n",
    "soup = bs(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"view-complete-item\" data-reactid=\"90\" href=\"/recipes/food/views/shaved-fennel-and-apple-salad-104117\" itemprop=\"url\" title=\"Shaved Fennel and Apple Salad\"><!-- react-text: 91 -->View “<!-- /react-text --><!-- react-text: 92 -->Shaved Fennel and Apple Salad<!-- /react-text --><!-- react-text: 93 -->”<!-- /react-text --></a>,\n",
       " <a class=\"view-complete-item\" data-reactid=\"95\" href=\"/recipes/food/views/shaved-fennel-and-apple-salad-104117\">View Recipe</a>,\n",
       " <a class=\"view-complete-item\" data-reactid=\"123\" href=\"/recipes/food/views/cranberry-orange-drop-cookies-107524\" itemprop=\"url\" title=\"Cranberry-Orange Drop Cookies\"><!-- react-text: 124 -->View “<!-- /react-text --><!-- react-text: 125 -->Cranberry-Orange Drop Cookies<!-- /react-text --><!-- react-text: 126 -->”<!-- /react-text --></a>,\n",
       " <a class=\"view-complete-item\" data-reactid=\"128\" href=\"/recipes/food/views/cranberry-orange-drop-cookies-107524\">View Recipe</a>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_links = soup.find_all('a', {'class': 'view-complete-item'})\n",
    "recipe_links[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/recipes/food/views/shaved-fennel-and-apple-salad-104117\n",
      "/recipes/food/views/cranberry-orange-drop-cookies-107524\n",
      "/recipes/food/views/whole-fish-baked-in-salt-104118\n",
      "/recipes/food/views/upside-down-caramelized-apricot-tart-105058\n",
      "/recipes/food/views/imperial-peach-sundaes-107892\n",
      "/recipes/food/views/black-olive-clafoutis-106771\n",
      "/recipes/food/views/herbed-lima-bean-hummus-103043\n",
      "/recipes/food/views/saffron-orzo-with-asparagus-and-prosciutto-107938\n",
      "/recipes/food/views/roasted-salted-pumpkin-seeds-103701\n",
      "/recipes/food/views/festive-tuna-salad-106704\n",
      "/recipes/food/views/xiao-jianmings-spareribs-with-chiles-107983\n",
      "/recipes/food/views/fettucine-with-smoked-salmon-and-asparagus-103187\n",
      "/recipes/food/views/mushroom-salad-with-endive-and-roquefort-cheese-105070\n",
      "/recipes/food/views/gratin-of-endive-and-ham-105062\n",
      "/recipes/food/views/broiled-sea-trout-with-basil-sauce-103815\n",
      "/recipes/food/views/black-sesame-rice-105033\n",
      "/recipes/food/views/watercress-radish-and-endive-salad-with-mustard-seed-vinaigrette-107912\n",
      "/recipes/food/views/lemongrass-sorbet-with-mango-105038\n"
     ]
    }
   ],
   "source": [
    "for link in recipe_links[0:len(recipe_links):2]: \n",
    "    print(link[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together\n",
    "\n",
    "Mind the multiprocessing constraint of not having a shared list, see https://stackoverflow.com/questions/49418926/append-to-the-same-list-with-multiprocessing-python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "base_urls = pickle.load(open(\"pickle/recipe_base_urls.pkl\", \"rb\"))\n",
    "base = \"https://www.epicurious.com\"\n",
    "\n",
    "#Function\n",
    "def crawl_pages(p, end_urls):\n",
    "    \n",
    "    #load page\n",
    "    print('Loading page: ', p)\n",
    "    page = urlopen(p)\n",
    "    soup = bs(page, 'html.parser')\n",
    "\n",
    "    #extract links\n",
    "    recipe_links = soup.find_all('a', {'class': 'view-complete-item'})\n",
    "    for link in recipe_links[0:len(recipe_links):2]: \n",
    "        end_urls.append(base+link[\"href\"])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    pool = mp.Pool(processes=2)\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    end_urls = manager.list()\n",
    "    [pool.apply_async(crawl_pages, args=[p,end_urls]) for p in base_urls]\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    with open('pickle/recipe_urls.pkl', 'wb') as f:\n",
    "        pickle.dump(list(end_urls), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22665"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = pickle.load(open(\"pickle/recipe_urls.pkl\", \"rb\"))\n",
    "print(len(my_list))\n",
    "\n",
    "my_list.index(\"https://www.epicurious.com/recipes/food/views/penne-with-roasted-tomatoes-chicken-and-mushrooms-107495\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "\n",
    "#### *Crawling the URLs for data* \n",
    "Testscript for crawler, further adjusted in *01_epicurious_crawler.py*. <br>Implentation in 16 processes on Google Cloud Ubuntu VM, runtime ~45min for all 35.776 recipes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on recipe: https://www.epicurious.com/recipes/food/views/crispy-scallop-salad\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/creamy-one-pot-pasta-with-sausage-and-squash\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/soy-glazed-chicken-with-broccoli\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/make-ahead-spanish-frittata\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/big-batch-rice\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/instant-pot-braised-lamb-with-white-beans-and-spinach\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/golden-noodles-with-chicken\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/winter-italian-chopped-salad\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/lentil-soup-with-wheat-berries-and-kale\n",
      "Working on recipe: https://www.epicurious.com/recipes/food/views/almond-butter-and-banana-pancakes\n"
     ]
    }
   ],
   "source": [
    "############ IMPORTS ###############\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict as dd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "############ RECIPE CLASS ###############\n",
    "\n",
    "class Recipe: \n",
    "    \n",
    "    #build recipe object from page\n",
    "    def __init__(self, page, url):\n",
    "        \n",
    "        self.title = self.get_title(page)\n",
    "        self.url = url\n",
    "        self.categories = self.get_categories(page)\n",
    "        self.desc = self.get_desc(page) \n",
    "        self.date = self.get_date(page)\n",
    "        self.image_link = self.get_image_link(page)\n",
    "        self.rating = self.get_rating(page)\n",
    "        self.recomm_perc = self.get_recomm_perc(page)\n",
    "        self.ingredients = self.get_ingredients(page)\n",
    "        self.nutrients = self.get_nutrients(page)\n",
    "        self.preparation = self.get_preparation(page)\n",
    "        self.preparation_note = self.get_chef_note(page)\n",
    "        self.servings = self.get_servings(page)\n",
    "        \n",
    "    #getters  \n",
    "    def get_title(self, page): \n",
    "        return page.find('h1', {'itemprop': 'name'}).text.strip()\n",
    "    \n",
    "    def get_date(self, page):\n",
    "        try:\n",
    "            return page.find('meta', {'itemprop': 'datePublished'})['content']\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_image_link(self, page): \n",
    "        try: \n",
    "            return page.find('img', {'class': 'photo loaded'})['srcset']\n",
    "        except: \n",
    "            return None\n",
    "    \n",
    "    def get_rating(self, page):\n",
    "        try:\n",
    "            return float(page.find('span', {'class': 'rating'}).text.split('/')[0])\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_recomm_perc(self, page):\n",
    "        try:\n",
    "            return float(page.find('div', {'class': 'prepare-again-rating'}).find('span').text[:-1])\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_desc(self, page):\n",
    "        try:\n",
    "            return page.find('div', {'itemprop': 'description'}).find('p').text.strip()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_ingredients(self, page):\n",
    "        return self.get_grouped_data(page, 'ingredient-group')\n",
    "    \n",
    "    def get_preparation(self, page):\n",
    "        return self.get_grouped_data(page, 'preparation-group')\n",
    "    \n",
    "    def get_chef_note(self, page):\n",
    "        try:\n",
    "            return page.find('div', {'class': 'chef-notes-content'}).text.strip()\n",
    "        except: \n",
    "            return None\n",
    "    \n",
    "    def get_servings(self,page): \n",
    "        try:\n",
    "            return int(page.find_all('span',{'class':'per-serving'})[0].text.split(\" \")[2][1:])\n",
    "        except: \n",
    "            return None\n",
    "    \n",
    "    def get_nutrients(self,page):\n",
    "        return self.get_nutridata(page)\n",
    "            \n",
    "    def get_categories(self, page):\n",
    "        return self.get_category_data(page)\n",
    "    \n",
    "    \n",
    "    #helpers\n",
    "    def get_grouped_data(self, page, group):\n",
    "        \n",
    "        groups = page.find_all('li', {'class': group})\n",
    "        \n",
    "        if len(groups) == 1:\n",
    "            return [i.text.strip() for i in groups[0].find_all('li')]\n",
    "\n",
    "        else:  \n",
    "            results = []\n",
    "            for i,g in enumerate(groups):\n",
    "                try: \n",
    "                    group_title = g.find('strong').text.strip(\":\")\n",
    "                except: \n",
    "                    group_title = \"Group {}\".format(i+1)\n",
    "                group_content = [i.text.strip() for i in g.find_all('li')]\n",
    "                results.append({group.replace(\"-\",\"_\"): group_title, \n",
    "                                group+\"-content\".replace(\"-\",\"_\"): group_content})\n",
    "            return results\n",
    "\n",
    "        \n",
    "    def get_nutridata(self, page): \n",
    "        \n",
    "        nutri_data = page.find_all('span', {'class':'nutri-data'}) \n",
    "        \n",
    "        if len(nutri_data) == 0: \n",
    "            return None\n",
    "        else: \n",
    "            nutrients = [\"calories\", \"carbohydrates\", \"fat\", \"protein\", \"saturated_fat\", \n",
    "                         \"sodium\", \"poly_fat\",\"fiber\",  \"mono_fat\", \"cholesterol\"]\n",
    "            list_of_10 = [float(n.text.split(\" \")[0]) if len(n.text) > 1 else None \n",
    "                          for n in nutri_data]\n",
    "            return {k:v for k,v in zip(nutrients,list_of_10)}\n",
    "        \n",
    "\n",
    "    def get_category_data(self, page):\n",
    "        \n",
    "        category_dict = dd(list)\n",
    "        \n",
    "        try:\n",
    "            for tag in page.find('dl', {'class': 'tags'}):\n",
    "                category_dict[\"{}\".format(tag[\"href\"].split(\"/\")[1])].append(tag.text)\n",
    "            return category_dict\n",
    "        except: \n",
    "            return None\n",
    "\n",
    "\n",
    "############ MAIN FUNCTIONS ###############\n",
    "\n",
    "#logs entry\n",
    "def log_entry(url,e): \n",
    "    with open('logs/crawler_log.txt', 'a') as logs: \n",
    "        logs.write(\"{} - Error at URL {}: {}\".format(str(dt.now(), url, e)))\n",
    "\n",
    "#takes a list and outputs even chunks\n",
    "def chunks(l, n):\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
    "\n",
    "#builds recipes\n",
    "def build_recipes(job_id, end_urls_slice):\n",
    "    \n",
    "    slice_recipes = []\n",
    "    for url in end_urls_slice:\n",
    "\n",
    "        print('Working on recipe: {}'.format(url))\n",
    "        try: \n",
    "            #process html\n",
    "            page_html = urlopen(url)\n",
    "            page = bs(page_html, 'html.parser')\n",
    "\n",
    "            #build & append object \n",
    "            recipe = Recipe(page, url)\n",
    "            slice_recipes.append(recipe.__dict__)\n",
    "        \n",
    "        except Exception as e: \n",
    "            log_entry(url, e)\n",
    "            continue\n",
    "    \n",
    "    #dumps one list for every process \n",
    "    with open('data/recipe_urls.json', 'a') as f:\n",
    "        json.dump(slice_recipes, f)\n",
    "    \n",
    "\n",
    "#handles multiprocessing \n",
    "def handle_jobs(data, job_number):\n",
    "    total = len(data)\n",
    "    chunk_size = int(total / job_number)\n",
    "    slices = chunks(data, chunk_size)\n",
    "    jobs = []\n",
    "        \n",
    "    for i, s in enumerate(slices):\n",
    "        j = mp.Process(target=build_recipes, args=(i, s))\n",
    "        jobs.append(j)\n",
    "    for j in jobs:\n",
    "        j.start()\n",
    "    \n",
    "\n",
    "#main\n",
    "def main(): \n",
    "    end_urls = pickle.load(open(\"pickle/recipe_urls_test.pkl\", \"rb\"))\n",
    "    handle_jobs(end_urls[:10], 2)\n",
    "    \n",
    "\n",
    "################ MAIN ###################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
